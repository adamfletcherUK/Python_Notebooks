{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Machine Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the train and test datasets\n",
    "train = pd.read_csv('/Users/adam//Downloads/titanic/train.csv')\n",
    "test = pd.read_csv('/Users/adam//Downloads/titanic/test.csv')\n",
    "\n",
    "# Store our passenger ID for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "As this notebook is designed to be used for classification ML. I have used the titanic dataset and feature engineering from another kaggle competitor so that I can focus on developing the ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = [train, test]\n",
    "\n",
    "# Some features of my own that I have added in\n",
    "# Gives the length of the name\n",
    "train['Name_length'] = train['Name'].apply(len)\n",
    "test['Name_length'] = test['Name'].apply(len)\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Feature engineering steps taken from Sina\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    \n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "    \n",
    "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "\n",
    "# Create a New feature CategoricalAge\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "    \n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colormap = plt.cm.RdBu\n",
    "#plt.figure(figsize=(14,12))\n",
    "#plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "#sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n",
    "#            square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove labels from data\n",
    "labels = np.array(train['Survived'])\n",
    "train = train.drop('Survived', axis = 1)\n",
    "\n",
    "\n",
    "#Split training dataset into train and validation sets\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(train, labels, test_size = 0.1, random_state = 42)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Validation Features Shape:', valid_features.shape)\n",
    "print('Validation Labels Shape:', valid_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make A Function for Classification ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_importance = lambda forest_class: forest_class.feature_importances_\n",
    "#svm_importance = lambda sup_class: sup_class.coef_[0] #TODO: Review coef_ docs\n",
    "\n",
    "NO_ESTIMATORS = 500\n",
    "\n",
    "classifiers = [\n",
    "    ['Random Forest', RandomForestClassifier(n_jobs = -1 , n_estimators = NO_ESTIMATORS, warm_start = True, max_depth= 6, max_features='sqrt'), 0],\n",
    "    ['Extra Trees', ExtraTreesClassifier(n_jobs= -1, n_estimators = NO_ESTIMATORS, max_depth = 8, min_samples_leaf = 2), 0],\n",
    "    ['AdaBoost', AdaBoostClassifier(n_estimators = NO_ESTIMATORS, learning_rate = 0.75), 0],\n",
    "    ['Gradient Boost', GradientBoostingClassifier(n_estimators = NO_ESTIMATORS, max_depth = 5, min_samples_leaf = 2), 0],\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "    ['SVC', SVC(kernel = 'linear', C = 0.025), 1],\n",
    "    ['XGBoost', xgb.XGBClassifier(n_estimators= NO_ESTIMATORS, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1), 0]\n",
    "]\n",
    "\n",
    "model_type = []\n",
    "for model in classifiers:\n",
    "    model_type.append(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of Models Used\n",
    "\n",
    "m1 = ['Random Forest', RandomForestClassifier(n_jobs = -1 , n_estimators = NO_ESTIMATORS, warm_start = True, max_depth= 6, max_features='sqrt'), 0]\n",
    "m2 = ['Extra Trees', ExtraTreesClassifier(n_jobs= -1, n_estimators = NO_ESTIMATORS, max_depth = 8, min_samples_leaf = 2), 0]\n",
    "m3 = ['AdaBoost', AdaBoostClassifier(n_estimators = NO_ESTIMATORS, learning_rate = 0.75), 0]\n",
    "m4 = ['Gradient Boost', GradientBoostingClassifier(n_estimators = NO_ESTIMATORS, max_depth = 5, min_samples_leaf = 2), 0]\n",
    "m5 = ['SVC', SVC(kernel = 'linear', C = 0.025), 1]\n",
    "m6 = ['XGBoost', xgb.XGBClassifier(n_estimators= NO_ESTIMATORS, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1), 0]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns = [model_type])\n",
    "df.append(m1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop That Does The ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall']\n",
    "accuracy_df = pd.DataFrame(index= scoring)\n",
    "importance_df = pd.DataFrame(index= train.columns)\n",
    "predictions = []\n",
    "CROSS_VALIDATION_NUMBER = 5\n",
    "##TODO: Add Random Seed Number\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print('Currently Modelling Using', classifier[0])\n",
    "    model = classifier[1].fit(train_features, train_labels)\n",
    "    predictions.append(model.predict(valid_features))\n",
    "    errors = abs(predictions - valid_labels)\n",
    "    \n",
    "    if classifier[2] == 0:\n",
    "        importance_df[classifier[0]] = classifier[1].feature_importances_\n",
    "    if classifier[2] == 1:\n",
    "        importance_df[classifier[0]] = abs(classifier[1].coef_[0])\n",
    "        \n",
    "    for i in scoring:\n",
    "        xval_score = cross_val_score(model, valid_features, valid_labels, cv=CROSS_VALIDATION_NUMBER, scoring=i)\n",
    "        accuracy_df.loc[i, classifier[0]] = np.mean(xval_score)\n",
    "        \n",
    "    ## ADD a field to store predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also look to add/remove features in a brute force approach to see the effect that has on feature importance\n",
    "\n",
    "##Â Plotting Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp = importance_df\n",
    "_temp.reset_index(level=0, inplace=True)\n",
    "_temp = _temp.rename(columns= {\"index\": \"Feature\"})\n",
    "_temp = pd.melt(_temp, id_vars=['Feature'], value_vars=model_type)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.factorplot(x='Feature', y='value', hue='variable', data=_temp, kind='bar', aspect = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Plotting Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_temp = accuracy_df\n",
    "_temp.reset_index(level=0, inplace=True)\n",
    "_temp = _temp.rename(columns= {\"index\": \"Method\"})\n",
    "_temp = pd.melt(_temp, id_vars=['Method'], value_vars=model_type)\n",
    "\n",
    "sns.factorplot(x='variable', y='value', hue='Method', data=_temp, kind='bar', aspect = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning: Voting on Predictions\n",
    "\n",
    "In order to vote on predictions, I need to have a system where the voting can grab each model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame\n",
    "\n",
    "\n",
    "for i in range(0, len(predictions)):\n",
    "    temp_df.append(predictions[i])\n",
    "    print(predictions[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "rf= RandomForestClassifier(n_jobs = -1 , n_estimators = NO_ESTIMATORS, warm_start = True, max_depth= 6, max_features='sqrt')\n",
    "et= ExtraTreesClassifier(n_jobs= -1, n_estimators = NO_ESTIMATORS, max_depth = 8, min_samples_leaf = 2)\n",
    "ab= AdaBoostClassifier(n_estimators = NO_ESTIMATORS, learning_rate = 0.75)\n",
    "gb= GradientBoostingClassifier(n_estimators = NO_ESTIMATORS, max_depth = 5, min_samples_leaf = 2)\n",
    "svc = SVC(kernel = 'linear', C = 0.025)\n",
    "xg = xgb.XGBClassifier(n_estimators= NO_ESTIMATORS, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1)\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(estimators=[rf, et], voting='hard')\n",
    "eclf1 = eclf1.fit(train_features, train_labels)\n",
    "print(eclf1.predict(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "clf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "eclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
    "\n",
    "model_type = []\n",
    "\n",
    "for i in classifiers:\n",
    "    model_type.append(i[1])\n",
    "\n",
    "#eclf1 = VotingClassifier(estimators=[model_type], voting='hard')\n",
    "\n",
    "\n",
    "eclf1 = eclf1.fit(train_features, train_labels)\n",
    "print('ECLF1')\n",
    "print(eclf1.predict(train_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
