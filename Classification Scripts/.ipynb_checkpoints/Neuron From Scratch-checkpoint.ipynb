{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # seeding for random number generation\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "    def sigmoid(self, x): \n",
    "        #applying the sigmoid function\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        #computing derivative to the Sigmoid function\n",
    "        # https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        #training the model to make accurate predictions while adjusting weights continually\n",
    "        for iteration in range(training_iterations):\n",
    "            #siphon the training data via  the neuron\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            #computing error rate for back-propagation\n",
    "            # TODO: Change to a sum of squares error\n",
    "            error = training_outputs - output\n",
    "            \n",
    "            #performing weight adjustments\n",
    "            # If the prediction is correct error = 0 so the weight remains the same\n",
    "            # Added in a learning rate to penalise the adjustments\n",
    "            adjustments = np.dot(training_inputs.T, learning_rate * (error * self.sigmoid_derivative(output)))\n",
    "            #ALTERNATIVE adjustments = np.dot(training_inputs.T, ())\n",
    "\n",
    "            self.synaptic_weights += adjustments\n",
    "\n",
    "    def think(self, inputs):\n",
    "        #passing the inputs via the neuron to get output   \n",
    "        #converting values to floats\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        output = self.sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #initializing the neuron class\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Beginning Randomly Generated Weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    #training data consisting of 4 examples--3 input values and 1 output\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    #training taking place\n",
    "    neural_network.train(training_inputs, training_outputs, 15000)\n",
    "\n",
    "    print(\"Ending Weights After Training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    user_input_one = str(input(\"User Input One: \"))\n",
    "    user_input_two = str(input(\"User Input Two: \"))\n",
    "    user_input_three = str(input(\"User Input Three: \"))\n",
    "    \n",
    "    print(\"Considering New Situation: \", user_input_one, user_input_two, user_input_three)\n",
    "    print(\"New Output data: \")\n",
    "    prediction = neural_network.think(np.array([user_input_one, user_input_two, user_input_three]))\n",
    "    \n",
    "    if prediction > 0.5:\n",
    "        inference = 1\n",
    "    else:\n",
    "        inference = 0\n",
    "        \n",
    "    print(prediction)\n",
    "    print(\"Predicted Value: \")\n",
    "    print(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "\n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
