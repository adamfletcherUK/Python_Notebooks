{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boolean-Matrix Factorization\n",
    "### Adam Fletcher\n",
    "\n",
    "## Using Nimfa Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import nimfa\n",
    "\n",
    "V = np.random.rand(40, 100)\n",
    "bmf = nimfa.Bmf(V, seed=\"nndsvd\", rank=10, max_iter=12, lambda_w=1.1, lambda_h=1.1)\n",
    "bmf_fit = bmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __fact_factor(X):\n",
    "    \"\"\"\n",
    "    Return dense factorization factor, so that output is printed nice if factor is sparse.\n",
    "     \n",
    "    :param X: Factorization factor.\n",
    "    :type X: :class:`scipy.sparse` of format csr, csc, coo, bsr, dok, lil, dia or :class:`numpy.matrix`\n",
    "    \"\"\"\n",
    "    return X.todense() if sp.isspmatrix(X) else X\n",
    "\n",
    "\n",
    "def print_info(fit, idx=None):\n",
    "    \"\"\"\n",
    "    Print to stdout info about the factorization.\n",
    "     \n",
    "    :param fit: Fitted factorization model.\n",
    "    :type fit: :class:`nimfa.models.mf_fit.Mf_fit`\n",
    "    :param idx: Name of the matrix (coefficient) matrix. Used only in the multiple NMF model. Therefore in factorizations \n",
    "                that follow standard or nonsmooth model, this parameter can be omitted. Currently, SNMNMF implements \n",
    "                multiple NMF model.\n",
    "    :type idx: `str` with values 'coef' or 'coef1' (`int` value of 0 or 1, respectively) \n",
    "    \"\"\"\n",
    "    print(\"=================================================================================================\")\n",
    "    print(\"Factorization method:\", fit.fit)\n",
    "    print(\"Initialization method:\", fit.fit.seed)\n",
    "    print(\"Basis matrix W: \")\n",
    "    print(__fact_factor(fit.basis()))\n",
    "    print(\"Mixture (Coefficient) matrix H%d: \" % (idx if idx != None else 0))\n",
    "    print(__fact_factor(fit.coef(idx)))\n",
    "    print(\"Distance (Euclidean): \", fit.distance(metric='euclidean', idx=idx))\n",
    "    # We can access actual number of iteration directly through fitted model.\n",
    "    # fit.fit.n_iter\n",
    "    print(\"Actual number of iterations: \", fit.summary(idx)['n_iter'])\n",
    "    # We can access sparseness measure directly through fitted model.\n",
    "    # fit.fit.sparseness()\n",
    "    print(\"Sparseness basis: %7.4f, Sparseness mixture: %7.4f\" % (fit.summary(idx)['sparseness'][0], fit.summary(idx)['sparseness'][1]))\n",
    "    # We can access explained variance directly through fitted model.\n",
    "    # fit.fit.evar()\n",
    "    print(\"Explained variance: \", fit.summary(idx)['evar'])\n",
    "    # We can access residual sum of squares directly through fitted model.\n",
    "    # fit.fit.rss()\n",
    "    print(\"Residual sum of squares: \", fit.summary(idx)['rss'])\n",
    "    # There are many more ... but just cannot print out everything =] and some measures need additional data or more runs\n",
    "    # e.g. entropy, predict, purity, coph_cor, consensus, select_features, score_features, connectivity\n",
    "    print(\"=================================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_info(bmf_fit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLX Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "seekers_data = pd.read_csv(\"./job_persona_data/seekers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleanup To Do:**\n",
    "- Turn True/False into 1's and 0's (Already a Bool)\n",
    "- What are the different platforms (Only Desktop) -- CAN REMOVE\n",
    "- top_persona is my label! (Though this is an unsupervised process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1): \n",
    "  \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "    # print list \n",
    "    for x in unique_list: \n",
    "        print(x)\n",
    "        \n",
    "unique(seekers_data['platform'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Unneeded Columns\n",
    "\n",
    "cleaned_seekers_data = seekers_data[[\n",
    "    #'user_id',\n",
    "    'survive',\n",
    "    'escape',\n",
    "    'quick_process',\n",
    "    'getting_inspired',\n",
    "    'structured_process',\n",
    "    'boost_my_cv',\n",
    "    'quick_scan',\n",
    "    'prove_to_self',\n",
    "    'compare_options',\n",
    "    'rich_data',\n",
    "    'unbiased_information',\n",
    "    'dry_data',\n",
    "    'clear_job_information',\n",
    "    'human_verification',\n",
    "    'head_start',\n",
    "    'treated_with_respect',\n",
    "    'standing_out',\n",
    "    'present_real_me',\n",
    "    'manage_insecurity',\n",
    "    'being_informed',\n",
    "    'learn_from_process',\n",
    "    'values',\n",
    "    'money',\n",
    "    'stability',\n",
    "    'atmosphere',\n",
    "    'balance',\n",
    "    'happiness',\n",
    "    'development',\n",
    "    'promotion',\n",
    "    'freedom',\n",
    "    'missed_interview',\n",
    "    'shortcutter',\n",
    "    'survivor',\n",
    "    'dreamer',\n",
    "    'prepared',\n",
    "    'perfect',\n",
    "    'believer'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_array = cleaned_seekers_data.to_numpy()\n",
    "seekers_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Nimfa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model\n",
    "\n",
    "seekers_bmf = nimfa.Bmf(\n",
    "    seekers_array, \n",
    "    seed=\"nndsvd\", \n",
    "    rank=8, \n",
    "    max_iter=50000, \n",
    "    lambda_w=1.1, \n",
    "    lambda_h=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_bmf_fit = seekers_bmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_info(seekers_bmf_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_W = __fact_factor(seekers_bmf_fit.basis())\n",
    "seekers_W = pd.DataFrame(seekers_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(seekers_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seekers_H = __fact_factor(seekers_bmf_fit.coef(idx= None))\n",
    "seekers_H = pd.DataFrame(seekers_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(seekers_array).to_csv('seekers_array.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Original Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(np.dot(seekers_W, seekers_H))\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.iloc[row][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10453 rows 37 columns\n",
    "rounded_predictions = predictions\n",
    "\n",
    "for column in rounded_predictions:\n",
    "    \n",
    "    for row in range(len(rounded_predictions)):\n",
    "        if rounded_predictions.iloc[row][column] >= 0.5:\n",
    "            rounded_predictions.iloc[row][column] = 1\n",
    "        else:\n",
    "            rounded_predictions.iloc[row][column] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = rounded_predictions - ((pd.DataFrame(seekers_array)) * 1)\n",
    "difference = difference.abs()\n",
    "#sns.heatmap(difference)\n",
    "#difference.head()\n",
    "diff_vector = difference.sum()\n",
    "print(\"Error Rate:\", (diff_vector.sum() / (10453 * 37)) * 100, \"%\"   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sci-kit Learn NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=6, init='random', random_state=0)\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(W).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(H).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.dot(W, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLX Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "model = NMF(n_components=6, init='nndsvda', random_state=0)\n",
    "\n",
    "\n",
    "scikit_seekers_W = model.fit_transform(seekers_array)\n",
    "scikit_seekers_H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scikit_seekers_W).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scikit_seekers_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Matrix and Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_predictions = pd.DataFrame(np.dot(scikit_seekers_W, scikit_seekers_H))\n",
    "scikit_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in scikit_predictions:\n",
    "    \n",
    "    for row in range(len(scikit_predictions)):\n",
    "        if scikit_predictions.iloc[row][column] >= 0.5:\n",
    "            scikit_predictions.iloc[row][column] = 1\n",
    "        else:\n",
    "            scikit_predictions.iloc[row][column] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = scikit_predictions - ((pd.DataFrame(seekers_array)) * 1)\n",
    "difference = difference.abs()\n",
    "#sns.heatmap(difference)\n",
    "#difference.head()\n",
    "diff_vector = difference.sum()\n",
    "print(\"Error Rate:\", (diff_vector.sum() / (10453 * 37)) * 100, \"%\"   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach\n",
    "# k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(\n",
    "    n_clusters=4, \n",
    "    random_state=0).fit(seekers_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components= 2)\n",
    "\n",
    "principal_components = pca.fit_transform(seekers_array)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principal_components\n",
    "             , columns = [\n",
    "                 'principal component 1', \n",
    "                 'principal component 2']) #,\n",
    "                 #'principal component 3'])\n",
    "\n",
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf['cluster'] = kmeans.labels_\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.pyplot.scatter(\n",
    "    principalDf['principal component 1'], \n",
    "    principalDf['principal component 2'], \n",
    "    s=None, \n",
    "    c=principalDf['cluster'],\n",
    "    cmap = 'Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "sequence_containing_x_vals = principalDf['principal component 1']\n",
    "sequence_containing_y_vals = principalDf['principal component 2']\n",
    "sequence_containing_z_vals = principalDf['principal component 3']\n",
    "\n",
    "\n",
    "ax.scatter(sequence_containing_x_vals, \n",
    "           sequence_containing_y_vals, \n",
    "           sequence_containing_z_vals,\n",
    "           c=principalDf['cluster'],\n",
    "           cmap = 'Set1')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import registers the 3D projection, but is otherwise unused.\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "import numpy as np\n",
    "\n",
    "fig = pyplot.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "sequence_containing_x_vals = principalDf['principal component 2']\n",
    "sequence_containing_y_vals = principalDf['principal component 3']\n",
    "sequence_containing_z_vals = principalDf['principal component 1']\n",
    "\n",
    "\n",
    "ax.scatter(sequence_containing_x_vals, \n",
    "           sequence_containing_y_vals, \n",
    "           sequence_containing_z_vals,\n",
    "           c=principalDf['cluster'],\n",
    "           cmap = 'Set1')\n",
    "ax.set_xlabel('PC2')\n",
    "ax.set_ylabel('PC3')\n",
    "ax.set_zlabel('PC1')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Clustering to Persona Identification\n",
    "\n",
    "Koos and Robin did research and analysis to identify personas using the site.\n",
    "In what capacity do the clusters and personas match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison_df = pd.DataFrame([\n",
    "#    seekers_data['user_id'], \n",
    "#    seekers_data['top_persona'], \n",
    "#    kmeans.labels_\n",
    "#]) \n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'User ID': seekers_data['user_id'],\n",
    "    'Top Persona': seekers_data['top_persona'],\n",
    "    'Cluster' : kmeans.labels_\n",
    "})\n",
    "comparison_df = comparison_df\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to count the number of times each cluster matched a persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.groupby('Top Persona')['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.groupby('Cluster')['Top Persona'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
