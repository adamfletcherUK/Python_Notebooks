{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Notebook\n",
    "**Adam Fletcher** <br>\n",
    "08 APR 19 <br>\n",
    "Version 1 <br>\n",
    "<br>\n",
    "Useful Links: <br>\n",
    "> https://uk.mathworks.com/help/stats/regression-and-anova.html\n",
    "\n",
    "**TODO:** \n",
    "- Table of contents\n",
    "- Can I make more generic?\n",
    "- Fix error warnings on single SVR\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- [Data Import](#Data_Import)\n",
    "\n",
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "import seaborn\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RANSACRegressor, LinearRegression, TheilSenRegressor\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge,Lasso,ElasticNet,BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Data_Import'></a>\n",
    "### Data Import\n",
    "Data should be imported with the __variable name = data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Import\n",
    "data = pd.read_excel('/Users/adam/Downloads/Concrete_Data.xls')\n",
    "data.columns = ['cement_component', 'furnace_slag', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate', 'fine_aggregate', 'age', 'concrete_strength']\n",
    "\n",
    "\n",
    "# Print info about the imported data\n",
    "print(\"Length of Data Frame:\", len(data), \"rows and\", len(data.columns), \"columns\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Test and Train Datasets\n",
    "\n",
    "70% of data is set as the training dataset with 30% being the testing dataset \n",
    "\n",
    "**Code Below:** 'concrete_strength' is the **dependent** variable. i.e the one we want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, featurex, featurey, train_index=0.7):\n",
    "\n",
    "    train, test = train_test_split(data, test_size = 1-train_index)\n",
    "\n",
    "    if type(feature) == list:\n",
    "        x_train = train[featurex].as_matrix()\n",
    "        y_train = train[featurey].as_matrix()\n",
    "\n",
    "        x_test = test[featurex].as_matrix()\n",
    "        y_test = test[featurey].as_matrix()\n",
    "\n",
    "    else:\n",
    "        x_train = [[x] for x in list(train[featurex])]\n",
    "        y_train = [[x] for x in list(train[featurey])]\n",
    "\n",
    "        x_test = [[x] for x in list(test[featurex])]\n",
    "        y_test = [[x] for x in list(test[featurey])]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Matrix\n",
    "\n",
    "Significant **positive-only** correlations (>= 0.1) shown in bold <br>\n",
    "**TODO:** Fix to also include negative correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,120))\n",
    "plot_count = 1\n",
    "\n",
    "for featurex in data.columns:\n",
    "    print('\\033[0m' + featurex.replace('_',' ').upper())\n",
    "    \n",
    "    for featurey in data.columns:\n",
    "        \n",
    "        if featurex == featurey:\n",
    "            plot_count = plot_count\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            data_tr = data[[featurex, featurey]]\n",
    "            data_tr=data_tr[(data_tr.T != 0).all()]\n",
    "\n",
    "            x_train, y_train, x_test, y_test = split_train_test(data_tr, featurex, featurey)\n",
    "\n",
    "            # Create linear regression object\n",
    "            regr = LinearRegression()\n",
    "\n",
    "            # Train the model using the training sets\n",
    "            regr.fit(x_train, y_train)\n",
    "            y_pred = regr.predict(x_test)\n",
    "\n",
    "            # Plot outputs\n",
    "            plt.subplot(28, len(data.columns),plot_count)\n",
    "\n",
    "            plt.scatter(x_test, y_test,  color='black')\n",
    "            plt.plot(x_test, y_pred, color='red', linewidth=3)\n",
    "            plt.xlabel(featurey.replace('_',' ').title())\n",
    "            plt.ylabel(featurex.replace('_',' ').title())\n",
    "            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.8, hspace=0.5) #Adjust the spacing between plots\n",
    "            if r2_score(y_test, y_pred) >= 0.1:\n",
    "                print ('\\033[1m' + \"R2:\", featurex,\"vs\",featurey,\"=\", r2_score(y_test, y_pred))\n",
    "            else:\n",
    "                print ('\\033[0m' + \"R2:\", featurex,\"vs\",featurey,\"=\", r2_score(y_test, y_pred))\n",
    "\n",
    "            plot_count+=1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Why do the R2 values change so much between itterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Linear Regression\n",
    "\n",
    "Replace **'concrete_strength'** with your **dependent variable** (i.e. the one we want to predict) <br>\n",
    "Replace **'features'** with your **test variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, feature, train_index=0.7):\n",
    "\n",
    "    train, test = train_test_split(data, test_size = 1-train_index)\n",
    "\n",
    "    if type(feature) == list:\n",
    "        x_train = train[feature].as_matrix()\n",
    "        y_train = train['concrete_strength'].as_matrix()\n",
    "\n",
    "        x_test = test[feature].as_matrix()\n",
    "        y_test = test['concrete_strength'].as_matrix()\n",
    "\n",
    "    else:\n",
    "        x_train = [[x] for x in list(train[feature])]\n",
    "        y_train = [[x] for x in list(train['concrete_strength'])]\n",
    "\n",
    "        x_test = [[x] for x in list(test[feature])]\n",
    "        y_test = [[x] for x in list(test['concrete_strength'])]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']\n",
    "\n",
    "data_tr = data\n",
    "data_tr=data_tr[(data_tr.T != 0).all()]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_train_test(data_tr, features)\n",
    "\n",
    "# Create linear regression object\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "\n",
    "plt.scatter(range(len(y_test)), y_test,  color='black')\n",
    "plt.plot(y_pred, color='blue', linewidth=3)\n",
    "\n",
    "print ('Features: %s'%str(features))\n",
    "print ('R2 score: %f'%r2_score(y_test, y_pred))\n",
    "print ('Intercept: %f'%regr.intercept_)\n",
    "print ('Coefficients: %s'%str(regr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge, Lasso or ElasticNet Regression\n",
    "\n",
    "Ridge Regression: model = Ridge()<br>\n",
    "Ridge regression seeks to add penalty scores to the residual of the sum of squares in regular linear regression.\n",
    "This allows the model to become more **robust to collinearity** (when two variables are highly correlated** \n",
    "\n",
    "Lasso Regression: model = Lasso() <br>\n",
    "\n",
    "ElasticNet: model = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = data\n",
    "data_tr=data_tr[(data_tr.T != 0).all()]\n",
    "\n",
    "x_train, y_train, x_test, y_test = split_train_test(data_tr, features)\n",
    "\n",
    "alphas = np.arange(0.1,5,0.1)\n",
    "\n",
    "model = Ridge() ## REPLACE REGRESSION TYPE HERE!!\n",
    "cv = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "\n",
    "y_pred = cv.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "plt.scatter(range(len(y_test)), y_test,  color='black')\n",
    "plt.plot(y_pred, color='blue', linewidth=3)\n",
    "\n",
    "print ('Features: %s'%str(features))\n",
    "print ('R2 score: %f'%r2_score(y_test, y_pred))\n",
    "print ('Intercept: %f'%regr.intercept_)\n",
    "print ('Coefficients: %s'%str(regr.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression\n",
    "\n",
    "Single Gradient Boosted\n",
    "> http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/ <br>\n",
    "> https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html\n",
    "\n",
    "**Overview:** <br>\n",
    "Gradient boosting is a method to fit multiple conditions to a regression problem. Working as a decision tree to scale weights of predictions with a learning rate to dampen the changes. The decision tree is then looped over to make many small steps in the right direction leading to a model with low variance. <br>\n",
    "- The initial prediction is that our dependent variable prediction is the mean of the distribution\n",
    "- The data is then grouped based on similar components (eg. gender, age)\n",
    "- The residual is then calculated (observed - predicted)\n",
    "- This is then multipled by the learning rate (between 0 - 1) **0.1 is usually pretty good**\n",
    "- This then used to calculate a new predicted value (mean + (learning rate * residual)\n",
    "    - Gives a small increase in accuracy over the mean\n",
    "- A second tree is then made based on the predictions of the first tree\n",
    "- Data is grouped based on categories and new residuals are calculated\n",
    "    - if multiple values are in one leaf/bucket the mean is taken for calculating residuals\n",
    "- For the predictions: becomes mean + (learning rate * 1st residual) + (learning rate * second residual)\n",
    "- The result is that many small steps of improvement are made in making the model more accuracy while still accounting for variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plot_count = 1\n",
    "\n",
    "for feature in ['cement_component', 'flay_ash', 'water_component', 'superplasticizer', 'coarse_aggregate']:\n",
    "    data_tr = data[['concrete_strength', feature]]\n",
    "    data_tr=data_tr[(data_tr.T != 0).all()]\n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_train_test(data_tr, feature)\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = GradientBoostingRegressor()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(x_train, y_train)\n",
    "    y_pred = regr.predict(x_test)\n",
    "\n",
    "    # Plot outputs\n",
    "    plt.subplot(2,3,plot_count)\n",
    "\n",
    "    plt.scatter(x_test, y_test,  color='black')\n",
    "    plt.plot(x_test, y_pred, color='blue',\n",
    "             linewidth=3)\n",
    "    plt.xlabel(feature.replace('_',' ').title())\n",
    "    plt.ylabel('Concrete strength')\n",
    "\n",
    "    print (feature, r2_score(y_test, y_pred))\n",
    "\n",
    "    plot_count+=1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)\n",
    "\n",
    "**Single Support Vector Regression**\n",
    "\n",
    "The below code is the same as the Linear regression except for:\n",
    "\n",
    "**regr = SVR(kernel='linear')**\n",
    "\n",
    "SVR uses a 'Kernel Trick' to transform non linear data to a higher dimentionality so that a linear line can be used to model the data.\n",
    "\n",
    "SVR(kernal= **'n'** ) n = ‘linear’, ‘poly’, ‘rbf’ (default), ‘sigmoid’, ‘precomputed’ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,120))\n",
    "plot_count = 1\n",
    "\n",
    "for featurex in data.columns:\n",
    "    print('\\033[0m' + featurex.replace('_',' ').upper())\n",
    "    \n",
    "    for featurey in data.columns:\n",
    "        \n",
    "        if featurex == featurey:\n",
    "            plot_count = plot_count\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            data_tr = data[[featurex, featurey]]\n",
    "            data_tr=data_tr[(data_tr.T != 0).all()]\n",
    "\n",
    "            x_train, y_train, x_test, y_test = split_train_test(data_tr, featurex, featurey)\n",
    "\n",
    "            # Create linear regression object\n",
    "            regr = SVR(kernel='linear')\n",
    "\n",
    "            # Train the model using the training sets\n",
    "            regr.fit(x_train, y_train)\n",
    "            y_pred = regr.predict(x_test)\n",
    "\n",
    "            # Plot outputs\n",
    "            plt.subplot(28, len(data.columns),plot_count)\n",
    "\n",
    "            plt.scatter(x_test, y_test,  color='black')\n",
    "            plt.plot(x_test, y_pred, color='red', linewidth=3)\n",
    "            plt.xlabel(featurey.replace('_',' ').title())\n",
    "            plt.ylabel(featurex.replace('_',' ').title())\n",
    "            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.8, hspace=0.5) #Adjust the spacing between plots\n",
    "            if r2_score(y_test, y_pred) >= 0.1:\n",
    "                print ('\\033[1m' + \"R2:\", featurex,\"vs\",featurey,\"=\", r2_score(y_test, y_pred))\n",
    "            else:\n",
    "                print ('\\033[0m' + \"R2:\", featurex,\"vs\",featurey,\"=\", r2_score(y_test, y_pred))\n",
    "\n",
    "            plot_count+=1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#LOTS OF ERROR WARNINGS!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
